# -*- coding: utf-8 -*-
"""Cancer Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uYQs1rSngiMi89Y-Hs8I1Vq9TCsFJMv7

## Breast Cancer Prediction Using Python
"""

from google.colab import drive
drive.mount('/content/drive')

# importing libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns

# reading data from the file
df=pd.read_csv("/content/drive/MyDrive/data.csv")

df.head()

df.info()

# return all the columns with null values count
df.isna().sum()

# return the size of dataset
df.shape

# remove the column
df=df.dropna(axis=1)

# shape of dataset after removing the null column
df.shape

# describe the dataset
df.describe()

# Get the count of malignant<M> and Benign<B> cells
df['diagnosis'].value_counts()

df['diagnosis'] = df['diagnosis'].astype('category')
sns.countplot(data=df, x='diagnosis')

# label encoding(convert the value of M and B into 1 and 0)
from sklearn.preprocessing import LabelEncoder
labelencoder_Y = LabelEncoder()
df.iloc[:,1]=labelencoder_Y.fit_transform(df.iloc[:,1].values)

df.head()

sns.pairplot(df.iloc[:,1:5],hue="diagnosis")

# get the correlation
df.iloc[:,1:32].corr()

# visualize the correlation
plt.figure(figsize=(10,10))
sns.heatmap(df.iloc[:,1:10].corr(),annot=True,fmt=".0%")

# split the dataset into dependent(X) and Independent(Y) datasets
X=df.iloc[:,2:31].values
Y=df.iloc[:,1].values

# spliting the data into trainning and test dateset
from sklearn.model_selection import train_test_split
X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.20,random_state=0)

# feature scaling
from sklearn.preprocessing import StandardScaler
X_train=StandardScaler().fit_transform(X_train)
X_test=StandardScaler().fit_transform(X_test)

def models(X_train, Y_train):
    # Logistic regression
    from sklearn.linear_model import LogisticRegression
    log = LogisticRegression(random_state=0)
    log.fit(X_train, Y_train)
        
    # Decision Tree
    from sklearn.tree import DecisionTreeClassifier
    tree = DecisionTreeClassifier(random_state=0, criterion="entropy")
    tree.fit(X_train, Y_train)
        
    # Random Forest
    from sklearn.ensemble import RandomForestClassifier
    forest = RandomForestClassifier(random_state=0, criterion="entropy", n_estimators=10)
    forest.fit(X_train, Y_train)
    
    # K-Nearest Neighbors
    from sklearn.neighbors import KNeighborsClassifier
    knn = KNeighborsClassifier(n_neighbors=5)
    knn.fit(X_train, Y_train)
    
    # Naive Bayes
    from sklearn.naive_bayes import GaussianNB
    nb = GaussianNB()
    nb.fit(X_train, Y_train)
        
    # SVM
    from sklearn.svm import SVC
    svm = SVC(kernel='linear', random_state=0)
    svm.fit(X_train, Y_train)

    # Print accuracies
    print('[0] Logistic regression accuracy:', log.score(X_train, Y_train))
    print('[1] Decision tree accuracy:', tree.score(X_train, Y_train))
    print('[2] Random forest accuracy:', forest.score(X_train, Y_train))
    print('[3] K-Nearest Neighbors accuracy:', knn.score(X_train, Y_train))
    print('[4] Naive Bayes accuracy:', nb.score(X_train, Y_train))
    print('[5] SVM accuracy:', svm.score(X_train, Y_train))

    # Return models
    return log, tree, forest, knn, nb, svm

model=models(X_train,Y_train)

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
model_names = [type(model[i]).__name__ for i in range(len(model))]
for i in range(len(model)):
    print("Model",i,":", model_names[i])
    Y_pred = model[i].predict(X_test)
    print("Confusion matrix:\n", confusion_matrix(Y_test, Y_pred))
    print(classification_report(Y_test, Y_pred))
    print("Accuracy: ", accuracy_score(Y_test, Y_pred))
    print()
    print()

import matplotlib.pyplot as plt

# Get accuracy scores for each model
accuracy_scores = []
for i in range(len(model)):
    Y_pred = model[i].predict(X_test)
    accuracy_scores.append(accuracy_score(Y_test, Y_pred))

# Plot bar chart with adjusted y-axis limits
model_names = [type(model[i]).__name__ for i in range(len(model))]
plt.bar(model_names, accuracy_scores)
plt.xticks(rotation=45)
plt.xlabel('Model')
plt.ylabel('Accuracy')
plt.title('Accuracy of Different Models')
plt.ylim(0.90, 1.0) # set the y-axis limits to zoom in
plt.show()

import matplotlib.pyplot as plt

# Get accuracy scores for each model
accuracy_scores = []
for i in range(len(model)):
    Y_pred = model[i].predict(X_test)
    accuracy_scores.append(accuracy_score(Y_test, Y_pred))

# Plot line chart
model_names = [type(model[i]).__name__ for i in range(len(model))]
plt.plot(model_names, accuracy_scores,'-o')
plt.xticks(rotation=45)
plt.xlabel('Model')
plt.ylabel('Accuracy')
plt.title('Accuracy of Different Models')
plt.show()

# prediction of random-forest
pred=model[2].predict(X_test)
print('Predicted values:')
print(pred)
print('Actual values:')
print(Y_test)

from joblib import dump
dump(model[2],"Cancer_prediction.joblib")